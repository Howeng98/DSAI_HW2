# -*- coding: utf-8 -*-
"""DSAI_hw2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1onwQhd_gAUpyE3MzhGK69vPdi1n83IzC
"""

#TODO
# 1.記得詢問助教是否是使用心得trainingdata和心得testingdata，如果是那就要做完善的洗資料，因為testingdata中就有一筆資料是有缺失的(blank)
# 2.詢問是否要用儲存成一個model，然後去跑助教的testingdata，並且在20天內得到最高的profit
# 3.重新判斷high,low和close收盤價的關係
# 4.改模型參數LSTM 256->64， batch_size, learning_rate, epochs, train_test_split_ratio, past_day&future_day

#TOFIX
# 1.每次讀到csv的時候，第一個資料都會被miss掉，去找出原因 
    # Fixed: 只要在read_csv裡面加入header=None就會讓api自動忽略把第一筆資料當做column項了
# 2.把缺失的資料，用predata 和 postdata 的平均補上。目前是直接使用前一筆data

!nvidia-smi

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM,Dense,Dropout
from keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from statistics import mean

main_path = 'drive/My Drive/Colab Notebooks/DSAI_HW2'
print(os.listdir(main_path))

train_df = pd.read_csv(os.path.join(main_path, 'training.csv'), header=None)
test_df = pd.read_csv(os.path.join(main_path, 'testing.csv'), header=None)

train_df.drop([1,2,3], inplace=True, axis=1)
test_df.drop([1,2,3], inplace=True, axis=1)

'''
# Replace Nan Values with prevalues and postvalues mean
rows_with_nan = [index for index, row in test_df.iterrows() if row.isnull().any()]
print('------Before------')
print(test_df.iloc[rows_with_nan])

# print(test_df[0][rows_with_nan[0]-1])
# print(test_df[0][rows_with_nan[0]+1])
for row in range(len(rows_with_nan)):  #actually only have 1 row data got Nan values
  for col in test_df.columns:
    if test_df[col].isnull().any(): #figure out which column got Nan
      try:
        test_df[col].replace(test_df[0][rows_with_nan[0]-1] , inplace=True)
      except:
        print('Predata or Postdata is not available, please check again!')
        raise
    else:
      print('xxx')
  # print(test_df[3][rows_with_nan])
  # test_df[0].fillna(0, inplace=True)

print('------After------')
'''
test_df.fillna(method='ffill', inplace=True)
# print(test_df.iloc[rows_with_nan])
# print(test_df.isnull().any().sum())



# Scaling
sc = MinMaxScaler(feature_range=(-1, 1))
scaled_train_df = sc.fit_transform(train_df)
scaled_test_df  = sc.transform(test_df)

# Variables
epochs = 100
batch_size = 32
past_day = 7
future_day = 1

#@title Default title text
# 以收盤價為train, 以開盤價為target label
def split_dataset(df, past_day, future_day):
  X, Y = [], []
  for i in range(len(df) - future_day - past_day):
    X.append(np.array(df[i:i+past_day, 0]))
    Y.append(np.array(df[i+past_day:i+past_day+future_day, 0]))
  return np.array(X), np.array(Y)

x_train, y_train = split_dataset(scaled_train_df, past_day, future_day)
x_test, y_test = split_dataset(scaled_test_df, past_day, future_day)

# print(x_train[0])
# print(y_train[0])
# print(x_train[1])
# print(y_train[1])
# print(x_train[2])
# print(y_train[2])

def build_model(shape):
  model = Sequential()
  model.add(LSTM(64, input_shape=(shape[1], shape[2]), return_sequences=True))
  model.add(Dropout(0.2))
  model.add(LSTM(64, return_sequences=True))
  model.add(Dropout(0.2))
  # model.add(LSTM(256, return_sequences=True))
  # model.add(Dropout(0.2))
  # model.add(LSTM(256, return_sequences=True))
  # model.add(Dropout(0.2))

  # model.add(TimeDistributed(Dense(1)))
  # model.add(Flatten())
  # model.add(Dense(5, activation='linear'))
  model.add(Dense(1))
  return model  
  
# Reshape the data into (Samples, Timestep, Features)
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# Build model
model = build_model(x_train.shape)
model.summary()


# Compile and Fit
reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)
model.compile(optimizer=Adam(), loss='mean_squared_error')
history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(x_test, y_test), shuffle=False, callbacks=[reduce_lr])

# Plotting
plt.figure(figsize=(11,6))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("Model Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train','Valid'])
plt.show()

print(x_test.shape)
print(x_test[0])
plt.figure(figsize=(20,8))
plt.plot(x_test[:,2,0])
plt.plot(y_test, color='red')
# plt.title("Open Price")
plt.title("Price")
plt.grid(True)
plt.legend(['close price','open price'])


print(y_test.shape)
print(y_test[0])
# plt.figure(figsize=(20,8))

# plt.grid(True)
# # plt.legend(['y_test'])

predicted = model.predict(x_test)

# for index,i in enumerate(train_df.columns):


# print(predicted.shape)
# print(predicted)
predict = sc.inverse_transform(predicted.reshape(predicted.shape[0], predicted.shape[1]))
print(predict.shape)
# print(predict)
# print(predict[0])

# print(y_test.shape)
# print(y_test[0])
plt.figure(figsize=(20,8))
plt.plot(sc.inverse_transform(y_test.reshape(-1,1)))
# plt.plot(predict)
plt.plot(predict[:,-1])
plt.legend(['y_test','predict'])
plt.grid(True)